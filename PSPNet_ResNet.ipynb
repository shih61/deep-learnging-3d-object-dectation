{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSPNet Model\n",
    "In this notebook, we develop the Pyramid Scene Parsing network (based on https://github.com/Lextal/pspnet-pytorch), along with ResNet pretrainerd model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-segmentation in /home/hsb2140/miniconda3/lib/python3.7/site-packages (0.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import glob\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Disable multiprocesing for numpy/opencv. We already multiprocess ourselves, this would mean every subprocess produces\n",
    "# even more threads which would lead to a lot of context switching, slowing things down a lot.\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import scipy.special\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset\n",
    "from lyft_dataset_sdk.utils.data_classes import LidarPointCloud, Box, Quaternion\n",
    "from lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix\n",
    "from lyft_dataset_sdk.eval.detection.mAP_evaluation import Box3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"car\", \"motorcycle\", \"bus\", \"bicycle\", \"truck\", \"pedestrian\", \"other_vehicle\", \"animal\", \"emergency_vehicle\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credits\n",
    "# Model implementation based on https://github.com/Lextal/pspnet-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "Here is some code for augmenting data. We are not using this yet, but we believe scaling, cropping, padding, changing color hues would be a great way to extend the data and improve model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numbers\n",
    "import math\n",
    "import collections\n",
    "\n",
    "from PIL import ImageOps, Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Padding:\n",
    "    def __init__(self, pad):\n",
    "        self.pad = pad\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return ImageOps.expand(img, border=self.pad, fill=0)\n",
    "\n",
    "\n",
    "class Scale:\n",
    "    def __init__(self, size, interpolation=Image.NEAREST):\n",
    "        assert isinstance(size, int) or (isinstance(size, collections.Iterable) and len(size) == 2)\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, imgmap):\n",
    "        img, target = imgmap\n",
    "        if isinstance(self.size, int):\n",
    "            w, h = img.size\n",
    "            if (w <= h and w == self.size) or (h <= w and h == self.size):\n",
    "                return img, target\n",
    "            if w < h:\n",
    "                ow = self.size\n",
    "                oh = int(self.size * h / w)\n",
    "                return img.resize((ow, oh), self.interpolation), target.resize((ow, oh), self.interpolation)\n",
    "            else:\n",
    "                oh = self.size\n",
    "                ow = int(self.size * w / h)\n",
    "                return img.resize((ow, oh), self.interpolation), target.resize((ow, oh), self.interpolation)\n",
    "        else:\n",
    "            return img.resize(self.size, self.interpolation), target.resize(self.size, self.interpolation)\n",
    "\n",
    "\n",
    "class CenterCrop:\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "    def __call__(self, imgmap):\n",
    "        img, target = imgmap\n",
    "        w, h = img.size\n",
    "        th, tw = self.size\n",
    "        x1 = int(round((w - tw) / 2.))\n",
    "        y1 = int(round((h - th) / 2.))\n",
    "        return img.crop((x1, y1, x1 + tw, y1 + th)), target.crop((x1, y1, x1 + tw, y1 + th))\n",
    "\n",
    "\n",
    "class RandomCrop:\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "    def __call__(self, imgmap):\n",
    "        img, target = imgmap\n",
    "        w, h = img.size\n",
    "        if self.size is not None:\n",
    "            th, tw = self.size\n",
    "            if w == tw and h == th:\n",
    "                return img, target\n",
    "            else:\n",
    "                x1 = random.randint(0, w - tw)\n",
    "                y1 = random.randint(0, h - th)\n",
    "            return img.crop((x1, y1, x1 + tw, y1 + th)), target.crop((x1, y1, x1 + tw, y1 + th))\n",
    "        else:\n",
    "            return img, target\n",
    "\n",
    "\n",
    "class RandomSizedCrop:\n",
    "\n",
    "    def __init__(self, size, interpolation=Image.NEAREST):\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, imgmap):\n",
    "        img, target = imgmap\n",
    "        for attempt in range(10):\n",
    "            area = img.size[0] * img.size[1]\n",
    "            target_area = random.uniform(0.5, 1.0) * area\n",
    "            aspect_ratio = random.uniform(3. / 4, 4. / 3)\n",
    "\n",
    "            w = int(round(math.sqrt(target_area * aspect_ratio)))\n",
    "            h = int(round(math.sqrt(target_area / aspect_ratio)))\n",
    "\n",
    "            if random.random() < 0.5:\n",
    "                w, h = h, w\n",
    "\n",
    "            if w <= img.size[0] and h <= img.size[1]:\n",
    "                x1 = random.randint(0, img.size[0] - w)\n",
    "                y1 = random.randint(0, img.size[1] - h)\n",
    "\n",
    "                img = img.crop((x1, y1, x1 + w, y1 + h))\n",
    "                target = target.crop((x1, y1, x1 + w, y1 + h))\n",
    "                assert(img.size == (w, h))\n",
    "                assert(target.size == (w, h))\n",
    "\n",
    "                return img.resize((self.size, self.size), self.interpolation), \\\n",
    "                       target.resize((self.size, self.size), self.interpolation)\n",
    "\n",
    "        # Fallback\n",
    "        scale = Scale(self.size, interpolation=self.interpolation)\n",
    "        crop = CenterCrop(self.size)\n",
    "        return crop(scale((img, target)))\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip:\n",
    "\n",
    "    def __call__(self, imgmap):\n",
    "        img, target = imgmap\n",
    "        if random.random() < 0.5:\n",
    "            return img.transpose(Image.FLIP_LEFT_RIGHT), target.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        return img, target\n",
    "\n",
    "\n",
    "class RandomRotation:\n",
    "\n",
    "    def __call__(self, imgmap, degree=10):\n",
    "        img, target = imgmap\n",
    "        deg = np.random.randint(-degree, degree, 1)[0]\n",
    "        return img.rotate(deg), target.rotate(deg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader\n",
    "We have already prepared our dataset. For local training and testing, we have split our training set 50/50, into training and validation. This allows us to validate our results locally and have a smaller dataset to train against.\n",
    "\n",
    "Once we are certain, we will train our model on the full dataset for Kaggke submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1152x576 with 0 Axes>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1152x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "# Use for Full training\n",
    "# train_data_folder = '/home/hsb2140/deep-learnging-3d-object-dectation/input/lyft3d-mask-test-data/bev_train_data'\n",
    "\n",
    "# Use for Local training (train/validation) split 50/50 - data already split 50/50 and stored\n",
    "train_data_folder = '/home/hsb2140/artifacts/bev_train_data'\n",
    "\n",
    "class BEVImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_filepaths, target_filepaths, map_filepaths=None):\n",
    "        self.input_filepaths = input_filepaths\n",
    "        self.target_filepaths = target_filepaths\n",
    "        self.map_filepaths = map_filepaths\n",
    "        \n",
    "        if map_filepaths is not None:\n",
    "            assert len(input_filepaths) == len(map_filepaths)\n",
    "        \n",
    "        assert len(input_filepaths) == len(target_filepaths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_filepath = self.input_filepaths[idx]\n",
    "        target_filepath = self.target_filepaths[idx]\n",
    "        \n",
    "        sample_token = input_filepath.split(\"/\")[-1].replace(\"_input.png\",\"\")\n",
    "        \n",
    "        im = cv2.imread(input_filepath, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        if self.map_filepaths:\n",
    "            map_filepath = self.map_filepaths[idx]\n",
    "            map_im = cv2.imread(map_filepath, cv2.IMREAD_UNCHANGED)\n",
    "            im = np.concatenate((im, map_im), axis=2)\n",
    "        \n",
    "        target = cv2.imread(target_filepath, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        im = im.astype(np.float32)/255\n",
    "        target = target.astype(np.int64)\n",
    "        \n",
    "        im = torch.from_numpy(im.transpose(2,0,1))\n",
    "        target = torch.from_numpy(target)\n",
    "        \n",
    "        return im, target, sample_token\n",
    "\n",
    "input_filepaths = sorted(glob.glob(os.path.join(train_data_folder, \"*_input.png\")))\n",
    "target_filepaths = sorted(glob.glob(os.path.join(train_data_folder, \"*_target.png\")))\n",
    "\n",
    "train_dataset = BEVImageDataset(input_filepaths, target_filepaths)\n",
    "    \n",
    "im, target, sample_token = train_dataset[1]\n",
    "im = im.numpy()\n",
    "target = target.numpy()\n",
    "\n",
    "plt.figure(figsize=(16,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import model_zoo\n",
    "from torchvision.models.densenet import densenet121, densenet161\n",
    "from torchvision.models.squeezenet import squeezenet1_1\n",
    "\n",
    "\n",
    "def load_weights_sequential(target, source_state):\n",
    "    model_to_load= {k: v for k, v in source_state.items() if k in target.state_dict().keys()}\n",
    "    target.load_state_dict(model_to_load)\n",
    "\n",
    "'''\n",
    "    Implementation of dilated ResNet-101 with deep supervision. Downsampling is changed to 8x\n",
    "'''\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, dilation=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, dilation=dilation, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride=stride, dilation=dilation)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes, stride=1, dilation=dilation)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, dilation=dilation,\n",
    "                               padding=dilation, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers=(3, 4, 23, 3)):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = [block(self.inplanes, planes, stride, downsample)]\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, dilation=dilation))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x_3 = self.layer3(x)\n",
    "        x = self.layer4(x_3)\n",
    "\n",
    "        return x, x_3\n",
    "\n",
    "\n",
    "'''\n",
    "    Implementation of DenseNet with deep supervision. Downsampling is changed to 8x \n",
    "'''\n",
    "\n",
    "\n",
    "class _DenseLayer(nn.Sequential):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm.1', nn.BatchNorm2d(num_input_features)),\n",
    "        self.add_module('relu.1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv.1', nn.Conv2d(num_input_features, bn_size *\n",
    "                                            growth_rate, kernel_size=1, stride=1, bias=False)),\n",
    "        self.add_module('norm.2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
    "        self.add_module('relu.2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv.2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
    "                                            kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_features = super(_DenseLayer, self).forward(x)\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.Sequential):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features, downsample=True):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
    "                                          kernel_size=1, stride=1, bias=False))\n",
    "        if downsample:\n",
    "            self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "        else:\n",
    "            self.add_module('pool', nn.AvgPool2d(kernel_size=1, stride=1))  # compatibility hack\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64, bn_size=4, drop_rate=0, pretrained=True):\n",
    "\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        # First convolution\n",
    "        self.start_features = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n",
    "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
    "            ('relu0', nn.ReLU(inplace=True)),\n",
    "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
    "        ]))\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "\n",
    "        init_weights = list(densenet121(pretrained=True).features.children())\n",
    "        start = 0\n",
    "        for i, c in enumerate(self.start_features.children()):\n",
    "            if pretrained:\n",
    "                c.load_state_dict(init_weights[i].state_dict())\n",
    "            start += 1\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n",
    "                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "            if pretrained:\n",
    "                block.load_state_dict(init_weights[start].state_dict())\n",
    "            start += 1\n",
    "            self.blocks.append(block)\n",
    "            setattr(self, 'denseblock%d' % (i + 1), block)\n",
    "\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                downsample = i < 1\n",
    "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2,\n",
    "                                    downsample=downsample)\n",
    "                if pretrained:\n",
    "                    trans.load_state_dict(init_weights[start].state_dict())\n",
    "                start += 1\n",
    "                self.blocks.append(trans)\n",
    "                setattr(self, 'transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.start_features(x)\n",
    "        deep_features = None\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            out = block(out)\n",
    "            if i == 5:\n",
    "                deep_features = out\n",
    "\n",
    "        return out, deep_features\n",
    "\n",
    "\n",
    "class Fire(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, squeeze_planes,\n",
    "                 expand1x1_planes, expand3x3_planes, dilation=1):\n",
    "        super(Fire, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
    "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
    "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
    "                                   kernel_size=1)\n",
    "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
    "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
    "                                   kernel_size=3, padding=dilation, dilation=dilation)\n",
    "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze_activation(self.squeeze(x))\n",
    "        return torch.cat([\n",
    "            self.expand1x1_activation(self.expand1x1(x)),\n",
    "            self.expand3x3_activation(self.expand3x3(x))\n",
    "        ], 1)\n",
    "\n",
    "\n",
    "class SqueezeNet(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained=False):\n",
    "        super(SqueezeNet, self).__init__()\n",
    "\n",
    "        self.feat_1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.feat_2 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            Fire(64, 16, 64, 64),\n",
    "            Fire(128, 16, 64, 64)\n",
    "        )\n",
    "        self.feat_3 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            Fire(128, 32, 128, 128, 2),\n",
    "            Fire(256, 32, 128, 128, 2)\n",
    "        )\n",
    "        self.feat_4 = nn.Sequential(\n",
    "            Fire(256, 48, 192, 192, 4),\n",
    "            Fire(384, 48, 192, 192, 4),\n",
    "            Fire(384, 64, 256, 256, 4),\n",
    "            Fire(512, 64, 256, 256, 4)\n",
    "        )\n",
    "        if pretrained:\n",
    "            weights = squeezenet1_1(pretrained=True).features.state_dict()\n",
    "            load_weights_sequential(self, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f1 = self.feat_1(x)\n",
    "        f2 = self.feat_2(f1)\n",
    "        f3 = self.feat_3(f2)\n",
    "        f4 = self.feat_4(f3)\n",
    "        return f4, f3\n",
    "\n",
    "\n",
    "'''\n",
    "    Handy methods for construction\n",
    "'''\n",
    "\n",
    "\n",
    "def squeezenet(pretrained=True):\n",
    "    return SqueezeNet(pretrained)\n",
    "\n",
    "\n",
    "def densenet(pretrained=True):\n",
    "    return DenseNet(pretrained=pretrained)\n",
    "\n",
    "\n",
    "def resnet18(pretrained=True):\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "    if pretrained:\n",
    "        load_weights_sequential(model, model_zoo.load_url(model_urls['resnet18']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet34(pretrained=True):\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "    if pretrained:\n",
    "        load_weights_sequential(model, model_zoo.load_url(model_urls['resnet34']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50(pretrained=True):\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "    if pretrained:\n",
    "        load_weights_sequential(model, model_zoo.load_url(model_urls['resnet50']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet101(pretrained=True):\n",
    "    model = ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "    if pretrained:\n",
    "        load_weights_sequential(model, model_zoo.load_url(model_urls['resnet101']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet152(pretrained=True):\n",
    "    model = ResNet(Bottleneck, [3, 8, 36, 3])\n",
    "    if pretrained:\n",
    "        load_weights_sequential(model, model_zoo.load_url(model_urls['resnet152']))\n",
    "    return model\n",
    "\n",
    "extractors = {}\n",
    "extractors['squeezenet'] = squeezenet\n",
    "extractors['densenet'] = densenet\n",
    "extractors['resnet18'] = resnet18\n",
    "extractors['resnet34'] = resnet34\n",
    "extractors['resnet50'] = resnet50\n",
    "extractors['resnet101'] = resnet101\n",
    "extractors['resnet152'] = resnet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class PSPModule(nn.Module):\n",
    "    def __init__(self, features, out_features=1024, sizes=(1, 2, 3, 6)):\n",
    "        super().__init__()\n",
    "        self.stages = []\n",
    "        self.stages = nn.ModuleList([self._make_stage(features, size) for size in sizes])\n",
    "        self.bottleneck = nn.Conv2d(features * (len(sizes) + 1), out_features, kernel_size=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def _make_stage(self, features, size):\n",
    "        prior = nn.AdaptiveAvgPool2d(output_size=(size, size))\n",
    "        conv = nn.Conv2d(features, features, kernel_size=1, bias=False)\n",
    "        return nn.Sequential(prior, conv)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        h, w = feats.size(2), feats.size(3)\n",
    "        priors = [F.upsample(input=stage(feats), size=(h, w), mode='bilinear') for stage in self.stages] + [feats]\n",
    "        bottle = self.bottleneck(torch.cat(priors, 1))\n",
    "        return self.relu(bottle)\n",
    "\n",
    "\n",
    "class PSPUpsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = 2 * x.size(2), 2 * x.size(3)\n",
    "        p = F.upsample(input=x, size=(h, w), mode='bilinear')\n",
    "        return self.conv(p)\n",
    "\n",
    "\n",
    "class PSPNet(nn.Module):\n",
    "    def __init__(self, n_classes=18, sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet34',\n",
    "                 pretrained=True):\n",
    "        super().__init__()\n",
    "#         self.feats = getattr(extractors, backend)(pretrained)\n",
    "        self.feats = extractors[backend](pretrained)\n",
    "        self.psp = PSPModule(psp_size, 1024, sizes)\n",
    "        self.drop_1 = nn.Dropout2d(p=0.3)\n",
    "\n",
    "        self.up_1 = PSPUpsample(1024, 256)\n",
    "        self.up_2 = PSPUpsample(256, 64)\n",
    "        self.up_3 = PSPUpsample(64, 64)\n",
    "\n",
    "        self.drop_2 = nn.Dropout2d(p=0.15)\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(64, n_classes, kernel_size=1),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(deep_features_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        f, class_f = self.feats(x) \n",
    "        p = self.psp(f)\n",
    "        p = self.drop_1(p)\n",
    "\n",
    "        p = self.up_1(p)\n",
    "        p = self.drop_2(p)\n",
    "\n",
    "        p = self.up_2(p)\n",
    "        p = self.drop_2(p)\n",
    "\n",
    "        p = self.up_3(p)\n",
    "        p = self.drop_2(p)\n",
    "\n",
    "        auxiliary = F.adaptive_max_pool2d(input=class_f, output_size=(1, 1)).view(-1, class_f.size(1))\n",
    "\n",
    "        return self.final(p), self.classifier(auxiliary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This custom RAdm optimizer is borrowed from the baseline U-NET model; it allows us to more efficiently optimize without using up too much memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        \n",
    "        self.degenerated_to_sgd = degenerated_to_sgd\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    elif self.degenerated_to_sgd:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = -1\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "                elif step_size > 0:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# We weigh the loss for the 0 class lower to account for (some of) the big class imbalance.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class_weights = torch.from_numpy(np.array([0.2] + [1.0]*len(classes), dtype=np.float32))\n",
    "class_weights = class_weights.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "models = {\n",
    "    'squeezenet': lambda: PSPNet(n_classes=len(classes)+1, sizes=(1, 2, 3, 6), psp_size=512, deep_features_size=256, backend='squeezenet'),\n",
    "    'densenet': lambda: PSPNet(n_classes=len(classes)+1, sizes=(1, 2, 3, 6), psp_size=1024, deep_features_size=512, backend='densenet'),\n",
    "    'resnet18': lambda: PSPNet(n_classes=len(classes)+1, sizes=(1, 2, 3, 6), psp_size=512, deep_features_size=256, backend='resnet18'),\n",
    "    'resnet34': lambda: PSPNet(n_classes=len(classes)+1, sizes=(1, 2, 3, 6), psp_size=512, deep_features_size=256, backend='resnet34'),\n",
    "    'resnet50': lambda: PSPNet(n_classes=len(classes)+1, sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet50'),\n",
    "    'resnet101': lambda: PSPNet(n_classes=len(classes)+1, sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet101'),\n",
    "    'resnet152': lambda: PSPNet(n_classes=len(classes)+1, sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet152')\n",
    "}\n",
    "\n",
    "def build_network(snapshot, backend):\n",
    "    epoch = 0\n",
    "    backend = backend.lower()\n",
    "    net = models[backend]()\n",
    "    net = nn.DataParallel(net)\n",
    "    if snapshot is not None:\n",
    "        _, epoch = os.path.basename(snapshot).split('_')\n",
    "        epoch = int(epoch)\n",
    "        net.load_state_dict(torch.load(snapshot))\n",
    "        logging.info(\"Snapshot for epoch {} loaded from {}\".format(epoch, snapshot))\n",
    "    net = net.cuda()\n",
    "    return net, epoch\n",
    "\n",
    "\n",
    "backend='resnet34'\n",
    "snapshot=None\n",
    "crop_x=256 \n",
    "crop_y=256\n",
    "batch_size=16\n",
    "alpha=0.01\n",
    "epochs=15\n",
    "milestones='10,20,30'\n",
    "gpu='0'\n",
    "\n",
    "net, starting_epoch = build_network(snapshot, backend)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True, num_workers=os.cpu_count()*2)\n",
    "\n",
    "'''\n",
    "    To follow this training routine you need a DataLoader that yields the tuples of the following format:\n",
    "    (Bx3xHxW FloatTensor x, BxHxW LongTensor y, BxN LongTensor y_cls) where\n",
    "    x - batch of input images,\n",
    "    y - batch of groung truth seg maps,\n",
    "    y_cls - batch of 1D tensors of dimensionality N: N total number of classes, \n",
    "    y_cls[i, T] = 1 if class T is present in image i, 0 otherwise\n",
    "'''\n",
    "train_loader, class_weights, n_images = None, None, None\n",
    "\n",
    "all_losses = []\n",
    "optim = RAdam(net.parameters(), lr=1e-3)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934b4d3bce9d4a2da3acc4a5b807d035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1103), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsb2140/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py:2390: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/hsb2140/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py:2479: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "/home/hsb2140/miniconda3/lib/python3.7/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.24649781\n",
      "Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf2047359d849d28a6334ab1a4a6079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1103), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.019393962\n",
      "Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c64b467bef64e2686af12f81e59732b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1103), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.013151167\n",
      "Epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2e4d0e44cd4ec598a41dcc0f0dcdbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1103), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.010835841\n",
      "Epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e66413df4d1476696a4c4c8855e4d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1103), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.009487289\n",
      "Epoch 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f00c8bb1fc4f54beee29e19e58da9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1103), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-80105a761596>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mepoch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-7a606507008a>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(\"Epoch\", epoch)\n",
    "    \n",
    "    progress_bar = tqdm_notebook(dataloader)\n",
    "\n",
    "#     seg_criterion = nn.NLLLoss2d(weight=class_weights)\n",
    "#     cls_criterion = nn.BCEWithLogitsLoss(weight=class_weights)\n",
    "    epoch_losses = []\n",
    "    net.train()\n",
    "    for ii, (X, target, sample_ids) in enumerate(progress_bar):\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        X = X.to(device)  # [N, 3, H, W]\n",
    "        target = target.to(device)  # [N, H, W] with class indices (0, 1)\n",
    "        prediction, out_cls = net(X)  # [N, 2, H, W]\n",
    "        \n",
    "#         seg_loss, cls_loss = seg_criterion(out, y), cls_criterion(out_cls, y_cls)\n",
    "#         loss = seg_loss + alpha * cls_loss\n",
    "        loss = F.cross_entropy(prediction, target, weight=class_weights)\n",
    "    \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        epoch_losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    print(\"Loss:\", np.mean(epoch_losses))\n",
    "    all_losses.extend(epoch_losses)\n",
    "\n",
    "    checkpoint_filename = \"1_pspnet_checkpoint_epoch_{}.pth\".format(epoch)\n",
    "    checkpoint_filepath = os.path.join('/home/hsb2140/', checkpoint_filename)\n",
    "    torch.save(net.state_dict(), checkpoint_filepath)\n",
    "#     train_loss = np.mean(epoch_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f38caa8aa50>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f38ca51a210>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = [0.24649781,0.019393962,0.013151167,0.010835841, 0.009487289]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
