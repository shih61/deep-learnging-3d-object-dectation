{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSPNet Model\n",
    "In this notebook, we develop the Pyramid Scene Parsing network (based on https://github.com/Lextal/pspnet-pytorch), along with ResNet pretrainerd model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import glob\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Disable multiprocesing for numpy/opencv. We already multiprocess ourselves, this would mean every subprocess produces\n",
    "# even more threads which would lead to a lot of context switching, slowing things down a lot.\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import scipy.special\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset\n",
    "from lyft_dataset_sdk.utils.data_classes import LidarPointCloud, Box, Quaternion\n",
    "from lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix\n",
    "from lyft_dataset_sdk.eval.detection.mAP_evaluation import Box3D\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"car\", \"motorcycle\", \"bus\", \"bicycle\", \"truck\", \"pedestrian\", \"other_vehicle\", \"animal\", \"emergency_vehicle\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credits\n",
    "# Model implementation based on https://github.com/Lextal/pspnet-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "Here is some code for augmenting data. We are not using this yet, but we believe scaling, cropping, padding, changing color hues would be a great way to extend the data and improve model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numbers\n",
    "import math\n",
    "import collections\n",
    "\n",
    "from PIL import ImageOps, Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Padding:\n",
    "    def __init__(self, pad):\n",
    "        self.pad = pad\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return ImageOps.expand(img, border=self.pad, fill=0)\n",
    "\n",
    "\n",
    "class Scale:\n",
    "    def __init__(self, size, interpolation=Image.NEAREST):\n",
    "        assert isinstance(size, int) or (isinstance(size, collections.Iterable) and len(size) == 2)\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, imgmap):\n",
    "        img, target = imgmap\n",
    "        if isinstance(self.size, int):\n",
    "            w, h = img.size\n",
    "            if (w <= h and w == self.size) or (h <= w and h == self.size):\n",
    "                return img, target\n",
    "            if w < h:\n",
    "                ow = self.size\n",
    "                oh = int(self.size * h / w)\n",
    "                return img.resize((ow, oh), self.interpolation), target.resize((ow, oh), self.interpolation)\n",
    "            else:\n",
    "                oh = self.size\n",
    "                ow = int(self.size * w / h)\n",
    "                return img.resize((ow, oh), self.interpolation), target.resize((ow, oh), self.interpolation)\n",
    "        else:\n",
    "            return img.resize(self.size, self.interpolation), target.resize(self.size, self.interpolation)\n",
    "\n",
    "\n",
    "class CenterCrop:\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "    def __call__(self, imgmap):\n",
    "        img, target = imgmap\n",
    "        w, h = img.size\n",
    "        th, tw = self.size\n",
    "        x1 = int(round((w - tw) / 2.))\n",
    "        y1 = int(round((h - th) / 2.))\n",
    "        return img.crop((x1, y1, x1 + tw, y1 + th)), target.crop((x1, y1, x1 + tw, y1 + th))\n",
    "\n",
    "\n",
    "class RandomCrop:\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "    def __call__(self, imgmap):\n",
    "        img, target = imgmap\n",
    "        w, h = img.size\n",
    "        if self.size is not None:\n",
    "            th, tw = self.size\n",
    "            if w == tw and h == th:\n",
    "                return img, target\n",
    "            else:\n",
    "                x1 = random.randint(0, w - tw)\n",
    "                y1 = random.randint(0, h - th)\n",
    "            return img.crop((x1, y1, x1 + tw, y1 + th)), target.crop((x1, y1, x1 + tw, y1 + th))\n",
    "        else:\n",
    "            return img, target\n",
    "\n",
    "\n",
    "class RandomSizedCrop:\n",
    "\n",
    "    def __init__(self, size, interpolation=Image.NEAREST):\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, imgmap):\n",
    "        img, target = imgmap\n",
    "        for attempt in range(10):\n",
    "            area = img.size[0] * img.size[1]\n",
    "            target_area = random.uniform(0.5, 1.0) * area\n",
    "            aspect_ratio = random.uniform(3. / 4, 4. / 3)\n",
    "\n",
    "            w = int(round(math.sqrt(target_area * aspect_ratio)))\n",
    "            h = int(round(math.sqrt(target_area / aspect_ratio)))\n",
    "\n",
    "            if random.random() < 0.5:\n",
    "                w, h = h, w\n",
    "\n",
    "            if w <= img.size[0] and h <= img.size[1]:\n",
    "                x1 = random.randint(0, img.size[0] - w)\n",
    "                y1 = random.randint(0, img.size[1] - h)\n",
    "\n",
    "                img = img.crop((x1, y1, x1 + w, y1 + h))\n",
    "                target = target.crop((x1, y1, x1 + w, y1 + h))\n",
    "                assert(img.size == (w, h))\n",
    "                assert(target.size == (w, h))\n",
    "\n",
    "                return img.resize((self.size, self.size), self.interpolation), \\\n",
    "                       target.resize((self.size, self.size), self.interpolation)\n",
    "\n",
    "        # Fallback\n",
    "        scale = Scale(self.size, interpolation=self.interpolation)\n",
    "        crop = CenterCrop(self.size)\n",
    "        return crop(scale((img, target)))\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip:\n",
    "\n",
    "    def __call__(self, imgmap):\n",
    "        img, target = imgmap\n",
    "        if random.random() < 0.5:\n",
    "            return img.transpose(Image.FLIP_LEFT_RIGHT), target.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        return img, target\n",
    "\n",
    "\n",
    "class RandomRotation:\n",
    "\n",
    "    def __call__(self, imgmap, degree=10):\n",
    "        img, target = imgmap\n",
    "        deg = np.random.randint(-degree, degree, 1)[0]\n",
    "        return img.rotate(deg), target.rotate(deg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader\n",
    "We have already prepared our dataset. For local training and testing, we have split our training set to about 70/30, into training and validation. This allows us to validate our results locally and have a smaller dataset to train against.\n",
    "\n",
    "Once we are certain, we will train our model on the full dataset for Kaggke submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 category,\n",
      "18 attribute,\n",
      "4 visibility,\n",
      "18421 instance,\n",
      "10 sensor,\n",
      "148 calibrated_sensor,\n",
      "177789 ego_pose,\n",
      "180 log,\n",
      "180 scene,\n",
      "22680 sample,\n",
      "189504 sample_data,\n",
      "638179 sample_annotation,\n",
      "1 map,\n",
      "Done loading in 18.8 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 4.5 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "classes = [\"car\", \"motorcycle\", \"bus\", \"bicycle\", \"truck\", \"pedestrian\", \"other_vehicle\", \"animal\", \"emergency_vehicle\"]\n",
    "level5data = LyftDataset(data_path='/home/ys3152/train_dataset', json_path='/home/ys3152/train_data', verbose=True)\n",
    "class_heights = {'animal':0.51,'bicycle':1.44,'bus':3.44,'car':1.72,'emergency_vehicle':2.39,'motorcycle':1.59,\n",
    "                'other_vehicle':3.23,'pedestrian':1.78,'truck':3.44}\n",
    "ARTIFACTS_FOLDER = \"/home/hsb2140/deep-learnging-3d-object-dectation/input/lyft3d-mask-test-data/\"\n",
    "# ARTIFACTS_FOLDER = \"/home/hsb2140/artifacts/\"\n",
    "data_folder = os.path.join(ARTIFACTS_FOLDER, \"bev_train_data\")\n",
    "#level5data = LyftDataset(data_path='/home/ys3152/test_dataset', json_path='/home/ys3152/test_data', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [(level5data.get('sample', record['first_sample_token'])['timestamp'], record) for record in level5data.scene]\n",
    "\n",
    "entries = []\n",
    "\n",
    "for start_time, record in sorted(records):\n",
    "    start_time = level5data.get('sample', record['first_sample_token'])['timestamp'] / 1000000\n",
    "\n",
    "    token = record['token']\n",
    "    name = record['name']\n",
    "    date = datetime.utcfromtimestamp(start_time)\n",
    "    host = \"-\".join(record['name'].split(\"-\")[:2])\n",
    "    first_sample_token = record[\"first_sample_token\"]\n",
    "\n",
    "    entries.append((host, name, date, token, first_sample_token))\n",
    "            \n",
    "df = pd.DataFrame(entries, columns=[\"host\", \"scene_name\", \"date\", \"scene_token\", \"first_sample_token\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 40 train/validation split scene counts\n"
     ]
    }
   ],
   "source": [
    "validation_hosts = [\"host-a007\", \"host-a008\", \"host-a009\"]\n",
    "\n",
    "validation_df = df[df[\"host\"].isin(validation_hosts)]\n",
    "vi = validation_df.index\n",
    "train_df = df[~df.index.isin(vi)]\n",
    "\n",
    "print(len(train_df), len(validation_df), \"train/validation split scene counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c71136de8d41f494095b1d15e50160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=140), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of tokens= 17640\n"
     ]
    }
   ],
   "source": [
    "# sample_sub = pd.read_csv('../input/3d-object-detection-for-autonomous-vehicles/sample_submission.csv')\n",
    "all_sample_tokens,scene_len = [],[]\n",
    "for sample_token in tqdm_notebook(train_df.first_sample_token.values):\n",
    "    i = 0\n",
    "    while sample_token:\n",
    "        all_sample_tokens.append(sample_token)\n",
    "        sample = level5data.get(\"sample\", sample_token)\n",
    "        sample_token = sample[\"next\"]\n",
    "        i += 1\n",
    "    scene_len.append(i)\n",
    "#     print(len(all_sample_tokens[-1]))\n",
    "    \n",
    "print('Total number of tokens=',len(all_sample_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hsb2140/deep-learnging-3d-object-dectation/input/lyft3d-mask-test-data/bev_train_data\n",
      "17640\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "# class BEVImageDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, sample_tokens, data_folder, is_train=True, ignore_map=True):\n",
    "#         self.is_train = is_train\n",
    "#         self.sample_tokens = sample_tokens\n",
    "#         self.data_folder = data_folder\n",
    "#         self.ignore_map = ignore_map\n",
    "#     def __len__(self):\n",
    "#         return len(self.sample_tokens)\n",
    "#     def __getitem__(self, idx):\n",
    "        \n",
    "#         sample_token = self.sample_tokens[idx]\n",
    "#         # Get file path, assuming data already preprocessed and stored in data_folder\n",
    "#         input_filepath = os.path.join(self.data_folder,f\"{sample_token}_input.png\")\n",
    "        \n",
    "#         target = None\n",
    "# #         if self.is_train:\n",
    "#         target_filepath = os.path.join(self.data_folder,f\"{sample_token}_target.png\")\n",
    "#         target = cv2.imread(target_filepath, cv2.IMREAD_UNCHANGED)\n",
    "#         if target is None:\n",
    "#             print()\n",
    "#         target = target.astype(np.int64)\n",
    "#         target = torch.from_numpy(target)\n",
    "        \n",
    "#         im = cv2.imread(input_filepath, cv2.IMREAD_UNCHANGED)\n",
    "# #         if not self.ignore_map:\n",
    "# #             map_filepath = os.path.join(self.data_folder,f\"{sample_token}_map.png\")\n",
    "# #             map_im = cv2.imread(map_filepath, cv2.IMREAD_UNCHANGED)\n",
    "# #             # Concatenate image and map for network input\n",
    "# #             im = np.concatenate((im, map_im), axis=2)\n",
    "#         im = im.astype(np.float32)/255\n",
    "#         im = torch.from_numpy(im.transpose(2,0,1))\n",
    "        \n",
    "#         if target is None:\n",
    "#             print(sample_token, target_filepath)\n",
    "        \n",
    "# #         if not self.is_train:\n",
    "# #             return im, sample_token\n",
    "# #         else:\n",
    "#         return im, target, sample_token\n",
    "\n",
    "class BEVImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_filepaths, target_filepaths, map_filepaths=None):\n",
    "        self.input_filepaths = input_filepaths\n",
    "        self.target_filepaths = target_filepaths\n",
    "        self.map_filepaths = map_filepaths\n",
    "        \n",
    "        if map_filepaths is not None:\n",
    "            assert len(input_filepaths) == len(map_filepaths)\n",
    "        \n",
    "        assert len(input_filepaths) == len(target_filepaths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_filepath = self.input_filepaths[idx]\n",
    "        target_filepath = self.target_filepaths[idx]\n",
    "        \n",
    "        sample_token = input_filepath.split(\"/\")[-1].replace(\"_input.png\",\"\")\n",
    "        \n",
    "        im = cv2.imread(input_filepath, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        if self.map_filepaths:\n",
    "            map_filepath = self.map_filepaths[idx]\n",
    "            map_im = cv2.imread(map_filepath, cv2.IMREAD_UNCHANGED)\n",
    "            im = np.concatenate((im, map_im), axis=2)\n",
    "        \n",
    "        target = cv2.imread(target_filepath, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        im = im.astype(np.float32)/255\n",
    "        target = target.astype(np.int64)\n",
    "        \n",
    "        im = torch.from_numpy(im.transpose(2,0,1))\n",
    "        target = torch.from_numpy(target)\n",
    "        \n",
    "        return im, target, sample_token\n",
    "\n",
    "input_filepaths = sorted(glob.glob(os.path.join(data_folder, \"*_input.png\")))\n",
    "target_filepaths = sorted(glob.glob(os.path.join(data_folder, \"*_target.png\")))\n",
    "\n",
    "# train_data_folder = '/home/hsb2140/deep-learnging-3d-object-dectation/input/lyft3d-mask-test-data/bev_train_data'\n",
    "\n",
    "# Use for Local training (train/validation) split 50/50 - data already split 50/50 and stored\n",
    "# test_data_folder = '/home/hsb2140/deep-learnging-3d-object-dectation/input/lyft3d-mask-test-data/test_data/test_data'\n",
    "data_folder = '/home/hsb2140/deep-learnging-3d-object-dectation/input/lyft3d-mask-test-data/bev_train_data'\n",
    "print(data_folder)\n",
    "print(len(all_sample_tokens))\n",
    "batch_size=16\n",
    "# train_dataset = BEVImageDataset(all_sample_tokens, data_folder, is_train=True, ignore_map=True)\n",
    "train_dataset = BEVImageDataset(input_filepaths, target_filepaths)\n",
    "# train_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size, shuffle=False, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import model_zoo\n",
    "from torchvision.models.densenet import densenet121, densenet161\n",
    "from torchvision.models.squeezenet import squeezenet1_1\n",
    "\n",
    "\n",
    "def load_weights_sequential(target, source_state):\n",
    "    model_to_load= {k: v for k, v in source_state.items() if k in target.state_dict().keys()}\n",
    "    target.load_state_dict(model_to_load)\n",
    "\n",
    "'''\n",
    "    Implementation of dilated ResNet-101 with deep supervision. Downsampling is changed to 8x\n",
    "'''\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, dilation=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, dilation=dilation, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride=stride, dilation=dilation)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes, stride=1, dilation=dilation)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, dilation=dilation,\n",
    "                               padding=dilation, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers=(3, 4, 23, 3)):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = [block(self.inplanes, planes, stride, downsample)]\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, dilation=dilation))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x_3 = self.layer3(x)\n",
    "        x = self.layer4(x_3)\n",
    "\n",
    "        return x, x_3\n",
    "\n",
    "\n",
    "'''\n",
    "    Implementation of DenseNet with deep supervision. Downsampling is changed to 8x \n",
    "'''\n",
    "\n",
    "\n",
    "class _DenseLayer(nn.Sequential):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm.1', nn.BatchNorm2d(num_input_features)),\n",
    "        self.add_module('relu.1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv.1', nn.Conv2d(num_input_features, bn_size *\n",
    "                                            growth_rate, kernel_size=1, stride=1, bias=False)),\n",
    "        self.add_module('norm.2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
    "        self.add_module('relu.2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv.2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
    "                                            kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_features = super(_DenseLayer, self).forward(x)\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.Sequential):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features, downsample=True):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
    "                                          kernel_size=1, stride=1, bias=False))\n",
    "        if downsample:\n",
    "            self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "        else:\n",
    "            self.add_module('pool', nn.AvgPool2d(kernel_size=1, stride=1))  # compatibility hack\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64, bn_size=4, drop_rate=0, pretrained=True):\n",
    "\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        # First convolution\n",
    "        self.start_features = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n",
    "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
    "            ('relu0', nn.ReLU(inplace=True)),\n",
    "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
    "        ]))\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "\n",
    "        init_weights = list(densenet121(pretrained=True).features.children())\n",
    "        start = 0\n",
    "        for i, c in enumerate(self.start_features.children()):\n",
    "            if pretrained:\n",
    "                c.load_state_dict(init_weights[i].state_dict())\n",
    "            start += 1\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n",
    "                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "            if pretrained:\n",
    "                block.load_state_dict(init_weights[start].state_dict())\n",
    "            start += 1\n",
    "            self.blocks.append(block)\n",
    "            setattr(self, 'denseblock%d' % (i + 1), block)\n",
    "\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                downsample = i < 1\n",
    "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2,\n",
    "                                    downsample=downsample)\n",
    "                if pretrained:\n",
    "                    trans.load_state_dict(init_weights[start].state_dict())\n",
    "                start += 1\n",
    "                self.blocks.append(trans)\n",
    "                setattr(self, 'transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.start_features(x)\n",
    "        deep_features = None\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            out = block(out)\n",
    "            if i == 5:\n",
    "                deep_features = out\n",
    "\n",
    "        return out, deep_features\n",
    "\n",
    "\n",
    "class Fire(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, squeeze_planes,\n",
    "                 expand1x1_planes, expand3x3_planes, dilation=1):\n",
    "        super(Fire, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
    "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
    "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
    "                                   kernel_size=1)\n",
    "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
    "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
    "                                   kernel_size=3, padding=dilation, dilation=dilation)\n",
    "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze_activation(self.squeeze(x))\n",
    "        return torch.cat([\n",
    "            self.expand1x1_activation(self.expand1x1(x)),\n",
    "            self.expand3x3_activation(self.expand3x3(x))\n",
    "        ], 1)\n",
    "\n",
    "\n",
    "class SqueezeNet(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained=False):\n",
    "        super(SqueezeNet, self).__init__()\n",
    "\n",
    "        self.feat_1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.feat_2 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            Fire(64, 16, 64, 64),\n",
    "            Fire(128, 16, 64, 64)\n",
    "        )\n",
    "        self.feat_3 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            Fire(128, 32, 128, 128, 2),\n",
    "            Fire(256, 32, 128, 128, 2)\n",
    "        )\n",
    "        self.feat_4 = nn.Sequential(\n",
    "            Fire(256, 48, 192, 192, 4),\n",
    "            Fire(384, 48, 192, 192, 4),\n",
    "            Fire(384, 64, 256, 256, 4),\n",
    "            Fire(512, 64, 256, 256, 4)\n",
    "        )\n",
    "        if pretrained:\n",
    "            weights = squeezenet1_1(pretrained=True).features.state_dict()\n",
    "            load_weights_sequential(self, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f1 = self.feat_1(x)\n",
    "        f2 = self.feat_2(f1)\n",
    "        f3 = self.feat_3(f2)\n",
    "        f4 = self.feat_4(f3)\n",
    "        return f4, f3\n",
    "\n",
    "\n",
    "'''\n",
    "    Handy methods for construction\n",
    "'''\n",
    "\n",
    "\n",
    "def squeezenet(pretrained=True):\n",
    "    return SqueezeNet(pretrained)\n",
    "\n",
    "\n",
    "def densenet(pretrained=True):\n",
    "    return DenseNet(pretrained=pretrained)\n",
    "\n",
    "\n",
    "def resnet18(pretrained=True):\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "    if pretrained:\n",
    "        load_weights_sequential(model, model_zoo.load_url(model_urls['resnet18']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet34(pretrained=True):\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "    if pretrained:\n",
    "        load_weights_sequential(model, model_zoo.load_url(model_urls['resnet34']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50(pretrained=True):\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "    if pretrained:\n",
    "        load_weights_sequential(model, model_zoo.load_url(model_urls['resnet50']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet101(pretrained=True):\n",
    "    model = ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "    if pretrained:\n",
    "        load_weights_sequential(model, model_zoo.load_url(model_urls['resnet101']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet152(pretrained=True):\n",
    "    model = ResNet(Bottleneck, [3, 8, 36, 3])\n",
    "    if pretrained:\n",
    "        load_weights_sequential(model, model_zoo.load_url(model_urls['resnet152']))\n",
    "    return model\n",
    "\n",
    "extractors = {}\n",
    "extractors['squeezenet'] = squeezenet\n",
    "extractors['densenet'] = densenet\n",
    "extractors['resnet18'] = resnet18\n",
    "extractors['resnet34'] = resnet34\n",
    "extractors['resnet50'] = resnet50\n",
    "extractors['resnet101'] = resnet101\n",
    "extractors['resnet152'] = resnet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class PSPModule(nn.Module):\n",
    "    def __init__(self, features, out_features=1024, sizes=(1, 2, 3, 6)):\n",
    "        super().__init__()\n",
    "        self.stages = []\n",
    "        self.stages = nn.ModuleList([self._make_stage(features, size) for size in sizes])\n",
    "        self.bottleneck = nn.Conv2d(features * (len(sizes) + 1), out_features, kernel_size=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def _make_stage(self, features, size):\n",
    "        prior = nn.AdaptiveAvgPool2d(output_size=(size, size))\n",
    "        conv = nn.Conv2d(features, features, kernel_size=1, bias=False)\n",
    "        return nn.Sequential(prior, conv)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        h, w = feats.size(2), feats.size(3)\n",
    "        priors = [F.upsample(input=stage(feats), size=(h, w), mode='bilinear') for stage in self.stages] + [feats]\n",
    "        bottle = self.bottleneck(torch.cat(priors, 1))\n",
    "        return self.relu(bottle)\n",
    "\n",
    "\n",
    "class PSPUpsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = 2 * x.size(2), 2 * x.size(3)\n",
    "        p = F.upsample(input=x, size=(h, w), mode='bilinear')\n",
    "        return self.conv(p)\n",
    "\n",
    "\n",
    "class PSPNet(nn.Module):\n",
    "    def __init__(self, n_classes=18, sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet34',\n",
    "                 pretrained=True):\n",
    "        super().__init__()\n",
    "#         self.feats = getattr(extractors, backend)(pretrained)\n",
    "        self.feats = extractors[backend](pretrained)\n",
    "        self.psp = PSPModule(psp_size, 1024, sizes)\n",
    "        self.drop_1 = nn.Dropout2d(p=0.3)\n",
    "\n",
    "        self.up_1 = PSPUpsample(1024, 256)\n",
    "        self.up_2 = PSPUpsample(256, 64)\n",
    "        self.up_3 = PSPUpsample(64, 64)\n",
    "\n",
    "        self.drop_2 = nn.Dropout2d(p=0.15)\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(64, n_classes, kernel_size=1),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(deep_features_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        f, class_f = self.feats(x) \n",
    "        p = self.psp(f)\n",
    "        p = self.drop_1(p)\n",
    "\n",
    "        p = self.up_1(p)\n",
    "        p = self.drop_2(p)\n",
    "\n",
    "        p = self.up_2(p)\n",
    "        p = self.drop_2(p)\n",
    "\n",
    "        p = self.up_3(p)\n",
    "        p = self.drop_2(p)\n",
    "\n",
    "        auxiliary = F.adaptive_max_pool2d(input=class_f, output_size=(1, 1)).view(-1, class_f.size(1))\n",
    "\n",
    "        return self.final(p), self.classifier(auxiliary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# We weigh the loss for the 0 class lower to account for (some of) the big class imbalance.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class_weights = torch.from_numpy(np.array([0.2] + [1.0]*len(classes), dtype=np.float32))\n",
    "class_weights = class_weights.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from radamoptimizer import *\n",
    "\n",
    "models = {\n",
    "    'squeezenet': lambda: PSPNet(n_classes=len(classes)+1, sizes=(1, 2, 3, 6), psp_size=512, deep_features_size=256, backend='squeezenet'),\n",
    "    'densenet': lambda: PSPNet(n_classes=len(classes)+1, sizes=(1, 2, 3, 6), psp_size=1024, deep_features_size=512, backend='densenet'),\n",
    "    'resnet18': lambda: PSPNet(n_classes=len(classes)+1, sizes=(1, 2, 3, 6), psp_size=512, deep_features_size=256, backend='resnet18'),\n",
    "    'resnet34': lambda: PSPNet(n_classes=len(classes)+1, sizes=(1, 2, 3, 6), psp_size=512, deep_features_size=256, backend='resnet34'),\n",
    "    'resnet50': lambda: PSPNet(n_classes=len(classes)+1, sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet50'),\n",
    "    'resnet101': lambda: PSPNet(n_classes=len(classes)+1, sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet101'),\n",
    "    'resnet152': lambda: PSPNet(n_classes=len(classes)+1, sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet152')\n",
    "}\n",
    "\n",
    "def build_network(snapshot, backend):\n",
    "    epoch = 0\n",
    "    backend = backend.lower()\n",
    "    net = models[backend]()\n",
    "    net = nn.DataParallel(net)\n",
    "    if snapshot is not None:\n",
    "        _, epoch = os.path.basename(snapshot).split('_')\n",
    "        epoch = int(epoch)\n",
    "        net.load_state_dict(torch.load(snapshot))\n",
    "        logging.info(\"Snapshot for epoch {} loaded from {}\".format(epoch, snapshot))\n",
    "    net = net.cuda()\n",
    "    return net, epoch\n",
    "\n",
    "\n",
    "backend='resnet34'\n",
    "snapshot=None\n",
    "crop_x=256 \n",
    "crop_y=256\n",
    "batch_size=16\n",
    "alpha=0.01\n",
    "epochs=15\n",
    "milestones='10,20,30'\n",
    "gpu='0'\n",
    "\n",
    "net, starting_epoch = build_network(snapshot, backend)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True, num_workers=os.cpu_count()*2)\n",
    "\n",
    "'''\n",
    "    To follow this training routine you need a DataLoader that yields the tuples of the following format:\n",
    "    (Bx3xHxW FloatTensor x, BxHxW LongTensor y, BxN LongTensor y_cls) where\n",
    "    x - batch of input images,\n",
    "    y - batch of groung truth seg maps,\n",
    "    y_cls - batch of 1D tensors of dimensionality N: N total number of classes, \n",
    "    y_cls[i, T] = 1 if class T is present in image i, 0 otherwise\n",
    "'''\n",
    "train_loader, class_weights, n_images = None, None, None\n",
    "\n",
    "all_losses = []\n",
    "optim = RAdam(net.parameters(), lr=1e-3)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "TODO: Model fails with current data preprocessing because data has not been preprocessed to match the input of the network. **This is a work in progress.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1caf18fcb6f54d709fdf0183b05bb9cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1418), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.27603704\n",
      "Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09552b09996c44858b26a081d59af382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1418), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.019266352\n",
      "Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0072f859d99b4f00814ea5a33d63c0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1418), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.013024436\n",
      "Epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262fbc0de5ef407a987383f7ab81fb09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1418), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.010725773\n",
      "Epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f48731f1484ba99a13c42afc3024b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1418), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.009505657\n",
      "Epoch 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eade39828354c24bab3ae320822b27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1418), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.0085438015\n",
      "Epoch 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340064b9d08f446e8e0032f7847b496e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1418), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.007785001\n",
      "Epoch 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4764bf1999494ab6e874c38f57744f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1418), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.007099654\n",
      "Epoch 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f149611f1214165a8c940af3a026e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1418), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-72b3cd517cef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(\"Epoch\", epoch)\n",
    "    \n",
    "    progress_bar = tqdm_notebook(dataloader)\n",
    "\n",
    "#     seg_criterion = nn.NLLLoss2d(weight=class_weights)\n",
    "#     cls_criterion = nn.BCEWithLogitsLoss(weight=class_weights)\n",
    "    epoch_losses = []\n",
    "    net.train()\n",
    "    for ii, (X, target, sample_ids) in enumerate(progress_bar):\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        X = X.to(device)  # [N, 3, H, W]\n",
    "        target = target.to(device)  # [N, H, W] with class indices (0, 1)\n",
    "        prediction, out_cls = net(X)  # [N, 2, H, W]\n",
    "        \n",
    "#         seg_loss, cls_loss = seg_criterion(out, y), cls_criterion(out_cls, y_cls)\n",
    "#         loss = seg_loss + alpha * cls_loss\n",
    "        loss = F.cross_entropy(prediction, target, weight=class_weights)\n",
    "    \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        epoch_losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    print(\"Loss:\", np.mean(epoch_losses))\n",
    "    all_losses.extend(epoch_losses)\n",
    "\n",
    "    checkpoint_filename = \"2_pspnet_checkpoint_epoch_{}.pth\".format(epoch)\n",
    "    checkpoint_filepath = os.path.join('/home/hsb2140/', checkpoint_filename)\n",
    "    torch.save(net.state_dict(), checkpoint_filepath)\n",
    "#     train_loss = np.mean(epoch_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f162c152110>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f16276c7d50>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = [0.24649781,0.019393962,0.013151167,0.010835841, 0.009487289]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        n_classes=2,\n",
    "        depth=5,\n",
    "        wf=6,\n",
    "        padding=False,\n",
    "        batch_norm=False,\n",
    "        up_mode='upconv',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Implementation of\n",
    "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "        (Ronneberger et al., 2015)\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "        Using the default arguments will yield the exact version used\n",
    "        in the original paper\n",
    "        Args:\n",
    "            in_channels (int): number of input channels\n",
    "            n_classes (int): number of output channels\n",
    "            depth (int): depth of the network\n",
    "            wf (int): number of filters in the first layer is 2**wf\n",
    "            padding (bool): if True, apply padding such that the input shape\n",
    "                            is the same as the output.\n",
    "                            This may introduce artifacts\n",
    "            batch_norm (bool): Use BatchNorm after layers with an\n",
    "                               activation function\n",
    "            up_mode (str): one of 'upconv' or 'upsample'.\n",
    "                           'upconv' will use transposed convolutions for\n",
    "                           learned upsampling.\n",
    "                           'upsample' will use bilinear upsampling.\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        assert up_mode in ('upconv', 'upsample')\n",
    "        self.padding = padding\n",
    "        self.depth = depth\n",
    "        prev_channels = in_channels\n",
    "        self.down_path = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.down_path.append(\n",
    "                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.up_path = nn.ModuleList()\n",
    "        for i in reversed(range(depth - 1)):\n",
    "            self.up_path.append(\n",
    "                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        blocks = []\n",
    "        for i, down in enumerate(self.down_path):\n",
    "            x = down(x)\n",
    "            if i != len(self.down_path) - 1:\n",
    "                blocks.append(x)\n",
    "                x = F.max_pool2d(x, 2)\n",
    "\n",
    "        for i, up in enumerate(self.up_path):\n",
    "            x = up(x, blocks[-i - 1])\n",
    "\n",
    "        return self.last(x)\n",
    "\n",
    "\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        block = []\n",
    "\n",
    "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        self.block = nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        if up_mode == 'upconv':\n",
    "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n",
    "        elif up_mode == 'upsample':\n",
    "            self.up = nn.Sequential(\n",
    "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "                nn.Conv2d(in_size, out_size, kernel_size=1),\n",
    "            )\n",
    "\n",
    "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
    "\n",
    "    def center_crop(self, layer, target_size):\n",
    "        _, _, layer_height, layer_width = layer.size()\n",
    "        diff_y = (layer_height - target_size[0]) // 2\n",
    "        diff_x = (layer_width - target_size[1]) // 2\n",
    "        return layer[\n",
    "            :, :, diff_y : (diff_y + target_size[0]), diff_x : (diff_x + target_size[1])\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
    "        out = torch.cat([up, crop1], 1)\n",
    "        out = self.conv_block(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def get_unet_model(in_channels=6, num_output_classes=2):\n",
    "    model = UNet(in_channels=in_channels, n_classes=num_output_classes, wf=5, depth=4, padding=True, up_mode='upsample')\n",
    "    \n",
    "    # Optional, for multi GPU training and inference\n",
    "    model = nn.DataParallel(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10 = get_unet_model(num_output_classes=len(classes)+1)\n",
    "\n",
    "state = torch.load('/home/hsb2140/deep-learnging-3d-object-dectation//input/lyft3d-mask-test-data/unet_checkpoint_epoch_10.pth')\n",
    "model10.load_state_dict(state)\n",
    "model10 = model10.to(device)\n",
    "model10.eval();\n",
    "\n",
    "\n",
    "# model9 = get_unet_model(num_output_classes=len(classes)+1)\n",
    "\n",
    "# state = torch.load('/home/hsb2140/deep-learnging-3d-object-dectation//input/lyft3d-mask-test-data/unet_checkpoint_epoch_9.pth')\n",
    "# model9.load_state_dict(state)\n",
    "# model9 = model9.to(device)\n",
    "# model9.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "    \n",
    "    def __call__(self, x_psp, x_unet):\n",
    "        res = []\n",
    "        x_unet = x_unet.cuda()\n",
    "        x_psp = x_psp.cuda()\n",
    "        with torch.no_grad():\n",
    "            psp_pred, _cls = self.models[0](x_psp)\n",
    "            unet_pred = self.models[1](x_unet)\n",
    "            res.append(psp_pred)\n",
    "            res.append(unet_pred)\n",
    "#             for m, t in self.models:\n",
    "#                 if t == 'PSP':\n",
    "#                     prediction, cls = m(x)\n",
    "#                 else:\n",
    "#                     prediction = m(x)\n",
    "#                 res.append(prediction)\n",
    "        res = torch.stack(res)\n",
    "        return torch.mean(res, dim=0)\n",
    "    \n",
    "net7, _ = build_network(None, backend)\n",
    "state = torch.load('/home/hsb2140/2_pspnet_checkpoint_epoch_7.pth')\n",
    "net7.load_state_dict(state)\n",
    "net7 = net7.to(device)\n",
    "net7.eval();\n",
    "\n",
    "# net6, _ = build_network(None, backend)\n",
    "# state = torch.load('/home/hsb2140/2_pspnet_checkpoint_epoch_6.pth')\n",
    "# net6.load_state_dict(state)\n",
    "# net6 = net6.to(device)\n",
    "# net6.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([net7,model10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file instance.json missing, using empty list\n",
      "JSON file sample_annotation.json missing, using empty list\n",
      "9 category,\n",
      "17 attribute,\n",
      "4 visibility,\n",
      "0 instance,\n",
      "8 sensor,\n",
      "168 calibrated_sensor,\n",
      "219744 ego_pose,\n",
      "218 log,\n",
      "218 scene,\n",
      "27468 sample,\n",
      "219744 sample_data,\n",
      "0 sample_annotation,\n",
      "1 map,\n",
      "Done loading in 5.6 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 1.3 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "class_heights = {'animal':0.51,'bicycle':1.44,'bus':3.44,'car':1.72,'emergency_vehicle':2.39,'motorcycle':1.59,\n",
    "                'other_vehicle':3.23,'pedestrian':1.78,'truck':3.44}\n",
    "level5data = LyftDataset(data_path='/home/ys3152/test_dataset', json_path='/home/ys3152/test_data', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some hyperparameters we'll need to define for the system\n",
    "voxel_size = (0.4, 0.4, 1.5)\n",
    "z_offset = -2.0\n",
    "bev_shape = (336, 336, 3)\n",
    "\n",
    "# We scale down each box so they are more separated when projected into our coarse voxel space.\n",
    "box_scale = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [(level5data.get('sample', record['first_sample_token'])['timestamp'], record) for record in level5data.scene]\n",
    "\n",
    "entries = []\n",
    "\n",
    "for start_time, record in sorted(records):\n",
    "    start_time = level5data.get('sample', record['first_sample_token'])['timestamp'] / 1000000\n",
    "\n",
    "    token = record['token']\n",
    "    name = record['name']\n",
    "    date = datetime.utcfromtimestamp(start_time)\n",
    "    host = \"-\".join(record['name'].split(\"-\")[:2])\n",
    "    first_sample_token = record[\"first_sample_token\"]\n",
    "\n",
    "    entries.append((host, name, date, token, first_sample_token))\n",
    "            \n",
    "df = pd.DataFrame(entries, columns=[\"host\", \"scene_name\", \"date\", \"scene_token\", \"first_sample_token\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>host</th>\n",
       "      <th>scene_name</th>\n",
       "      <th>date</th>\n",
       "      <th>scene_token</th>\n",
       "      <th>first_sample_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>host-a007</td>\n",
       "      <td>host-a007-lidar0-1230678335199240106-123067836...</td>\n",
       "      <td>2019-01-04 23:05:35.302051</td>\n",
       "      <td>582583077b5db62b9b95d780c7fb3214f2fb6680ec61ef...</td>\n",
       "      <td>57b51d24fad1c39065f80c9f769b3ff8d29e17e0cf92f5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>host-a007</td>\n",
       "      <td>host-a007-lidar0-1230931253199029066-123093127...</td>\n",
       "      <td>2019-01-07 21:20:53.301923</td>\n",
       "      <td>795a318208bdc612f92eabe3af3102f9e55db4b6c1e44e...</td>\n",
       "      <td>780168b66b14e7f826b365d7f5f0ec602d70fc5df4edb9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>host-a007</td>\n",
       "      <td>host-a007-lidar0-1230939239197974066-123093926...</td>\n",
       "      <td>2019-01-07 23:33:59.300673</td>\n",
       "      <td>84ae53a19295800de1565ea5e61ef0d5e3d938a4b0601c...</td>\n",
       "      <td>99e78fc0682700b8be3635ef06231d813951d038e48795...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>host-a009</td>\n",
       "      <td>host-a009-lidar0-1231184014198521956-123118403...</td>\n",
       "      <td>2019-01-10 19:33:34.301170</td>\n",
       "      <td>225300b10634526aafdce9bb3e00e863b24663e6766041...</td>\n",
       "      <td>c88dfa9d87d41079250dd7488f9ee3721dfda7e5b45563...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>host-a008</td>\n",
       "      <td>host-a008-lidar0-1231272360198562866-123127238...</td>\n",
       "      <td>2019-01-11 20:06:00.301206</td>\n",
       "      <td>07b333dad30191a7d3048919c420083a7f53296b885038...</td>\n",
       "      <td>2299492f339a64c15d192e1b9ac3836917fb77660a4d72...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        host                                         scene_name  \\\n",
       "0  host-a007  host-a007-lidar0-1230678335199240106-123067836...   \n",
       "1  host-a007  host-a007-lidar0-1230931253199029066-123093127...   \n",
       "2  host-a007  host-a007-lidar0-1230939239197974066-123093926...   \n",
       "3  host-a009  host-a009-lidar0-1231184014198521956-123118403...   \n",
       "4  host-a008  host-a008-lidar0-1231272360198562866-123127238...   \n",
       "\n",
       "                        date  \\\n",
       "0 2019-01-04 23:05:35.302051   \n",
       "1 2019-01-07 21:20:53.301923   \n",
       "2 2019-01-07 23:33:59.300673   \n",
       "3 2019-01-10 19:33:34.301170   \n",
       "4 2019-01-11 20:06:00.301206   \n",
       "\n",
       "                                         scene_token  \\\n",
       "0  582583077b5db62b9b95d780c7fb3214f2fb6680ec61ef...   \n",
       "1  795a318208bdc612f92eabe3af3102f9e55db4b6c1e44e...   \n",
       "2  84ae53a19295800de1565ea5e61ef0d5e3d938a4b0601c...   \n",
       "3  225300b10634526aafdce9bb3e00e863b24663e6766041...   \n",
       "4  07b333dad30191a7d3048919c420083a7f53296b885038...   \n",
       "\n",
       "                                  first_sample_token  \n",
       "0  57b51d24fad1c39065f80c9f769b3ff8d29e17e0cf92f5...  \n",
       "1  780168b66b14e7f826b365d7f5f0ec602d70fc5df4edb9...  \n",
       "2  99e78fc0682700b8be3635ef06231d813951d038e48795...  \n",
       "3  c88dfa9d87d41079250dd7488f9ee3721dfda7e5b45563...  \n",
       "4  2299492f339a64c15d192e1b9ac3836917fb77660a4d72...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f104f523d94b8c808838880a1c28b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=218), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of tokens= 27468\n"
     ]
    }
   ],
   "source": [
    "# sample_sub = pd.read_csv('../input/3d-object-detection-for-autonomous-vehicles/sample_submission.csv')\n",
    "all_sample_tokens,scene_len = [],[]\n",
    "for sample_token in tqdm_notebook(df.first_sample_token.values):\n",
    "    i = 0\n",
    "    while sample_token:\n",
    "        all_sample_tokens.append(sample_token)\n",
    "        sample = level5data.get(\"sample\", sample_token)\n",
    "        sample_token = sample[\"next\"]\n",
    "        i += 1\n",
    "    scene_len.append(i)\n",
    "#     print(len(all_sample_tokens[-1]))\n",
    "    \n",
    "print('Total number of tokens=',len(all_sample_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "class BEVImageTestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sample_tokens, data_folder):\n",
    "        self.sample_tokens = sample_tokens\n",
    "        self.data_folder = data_folder\n",
    "    def __len__(self):\n",
    "        return len(self.sample_tokens)\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        sample_token = self.sample_tokens[idx]\n",
    "        input_filepath = os.path.join(self.data_folder,f\"{sample_token}_input.png\")\n",
    "        map_filepath = os.path.join(self.data_folder,f\"{sample_token}_map.png\")\n",
    "        \n",
    "        im = cv2.imread(input_filepath, cv2.IMREAD_UNCHANGED)\n",
    "        m_im = cv2.imread(input_filepath, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        map_im = cv2.imread(map_filepath, cv2.IMREAD_UNCHANGED)\n",
    "        m_im = np.concatenate((m_im, map_im), axis=2)\n",
    "        \n",
    "        im = im.astype(np.float32)/255\n",
    "        m_im = m_im.astype(np.float32)/255\n",
    "        im = torch.from_numpy(im.transpose(2,0,1))\n",
    "        m_im = torch.from_numpy(m_im.transpose(2,0,1))\n",
    "        \n",
    "        return im, m_im, sample_token\n",
    "\n",
    "test_data_folder = '/home/hsb2140/deep-learnging-3d-object-dectation/input/lyft3d-mask-test-data/test_data/test_data'\n",
    "\n",
    "test_dataset = BEVImageTestDataset(all_sample_tokens,test_data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "tensor([0.2000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0')\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# We weigh the loss for the 0 class lower to account for (some of) the big class imbalance.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class_weights = torch.from_numpy(np.array([0.2] + [1.0]*len(classes), dtype=np.float32))\n",
    "class_weights = class_weights.to(device)\n",
    "print(np.array([1.0]*len(classes), dtype=np.float32))\n",
    "print(class_weights)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d92db3ce6f744dfb6e602d4fcc26296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1717), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsb2140/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py:2390: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/hsb2140/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py:2479: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "/home/hsb2140/miniconda3/lib/python3.7/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3))\n",
    "\n",
    "net.eval();\n",
    "import gc\n",
    "gc.collect()\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size, shuffle=False, num_workers=os.cpu_count()*2)\n",
    "progress_bar = tqdm_notebook(test_loader)\n",
    "\n",
    "# We quantize to uint8 here to conserve memory. We're allocating >20GB of memory otherwise.\n",
    "# predictions = np.zeros((len(test_loader), 1+len(classes), 336, 336), dtype=np.uint8)\n",
    "\n",
    "sample_tokens = []\n",
    "all_losses = []\n",
    "\n",
    "detection_boxes = []\n",
    "detection_scores = []\n",
    "detection_classes = []\n",
    "\n",
    "# Arbitrary threshold in our system to create a binary image to fit boxes around.\n",
    "background_threshold = 200\n",
    "\n",
    "with torch.no_grad():\n",
    "    for ii, (X, X2, batch_sample_tokens) in enumerate(progress_bar):\n",
    "\n",
    "        sample_tokens.extend(batch_sample_tokens)\n",
    "        \n",
    "        X = X.to(device)  # [N, 1, H, W]\n",
    "        prediction = model(X, X2)  # [N, 2, H, W]\n",
    "        prediction = F.softmax(prediction, dim=1)\n",
    "        \n",
    "        prediction_cpu = prediction.cpu().numpy()\n",
    "        predictions = np.round(prediction_cpu*255).astype(np.uint8)\n",
    "        \n",
    "        # Get probabilities for non-background\n",
    "        predictions_non_class0 = 255 - predictions[:,0]\n",
    "        \n",
    "        predictions_opened = np.zeros((predictions_non_class0.shape), dtype=np.uint8)\n",
    "\n",
    "        for i, p in enumerate(predictions_non_class0):\n",
    "            thresholded_p = (p > background_threshold).astype(np.uint8)\n",
    "            predictions_opened[i] = cv2.morphologyEx(thresholded_p, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "            sample_boxes,sample_detection_scores,sample_detection_classes = calc_detection_box(predictions_opened[i],\n",
    "                                                                                              predictions[i])\n",
    "        \n",
    "            detection_boxes.append(np.array(sample_boxes))\n",
    "            detection_scores.append(sample_detection_scores)\n",
    "            detection_classes.append(sample_detection_classes)\n",
    "        \n",
    "#         # Visualize the first prediction\n",
    "#         if ii == 0:\n",
    "#             visualize_predictions(X, prediction, apply_softmaxiii=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of boxes: 622784\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Total amount of boxes:\", np.sum([len(x) for x in detection_boxes]))\n",
    "    \n",
    "\n",
    "# Visualize the boxes in the first sample\n",
    "t = np.zeros_like(predictions_opened[0])\n",
    "for sample_boxes in detection_boxes[0]:\n",
    "    box_pix = np.int0(sample_boxes)\n",
    "    cv2.drawContours(t,[box_pix],0,(255),2)\n",
    "plt.imshow(t)\n",
    "plt.show()\n",
    "\n",
    "# Visualize their probabilities\n",
    "plt.hist(detection_scores[0], bins=20)\n",
    "plt.xlabel(\"Detection Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Camera of Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ff93c548584761a84d69017179adc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=27468), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('/home/hsb2140/deep-learnging-3d-object-dectation/cam_viz',exist_ok=True)\n",
    " \n",
    "from lyft_dataset_sdk.eval.detection.mAP_evaluation import Box3D, recall_precision\n",
    "import shutil\n",
    "\n",
    "pred_box3ds = []\n",
    "\n",
    "max_frames = 128\n",
    "vid_count = 0\n",
    "processed_samples = 0\n",
    "for (sample_token, sample_boxes, sample_detection_scores, sample_detection_class) in tqdm_notebook(zip(sample_tokens, detection_boxes, detection_scores, detection_classes), total=len(sample_tokens)):\n",
    "    processed_samples += 1\n",
    "    sample_boxes = sample_boxes.reshape(-1, 2) # (N, 4, 2) -> (N*4, 2)\n",
    "    sample_boxes = sample_boxes.transpose(1,0) # (N*4, 2) -> (2, N*4)\n",
    "\n",
    "    # Add Z dimension\n",
    "    sample_boxes = np.vstack((sample_boxes, np.zeros(sample_boxes.shape[1]),)) # (2, N*4) -> (3, N*4)\n",
    "\n",
    "    sample = level5data.get(\"sample\", sample_token)\n",
    "    sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n",
    "    lidar_data = level5data.get(\"sample_data\", sample_lidar_token)\n",
    "    lidar_filepath = level5data.get_sample_data_path(sample_lidar_token)\n",
    "    ego_pose = level5data.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "    ego_translation = np.array(ego_pose['translation'])\n",
    "\n",
    "    global_from_car = transform_matrix(ego_pose['translation'],\n",
    "                                       Quaternion(ego_pose['rotation']), inverse=False)\n",
    "\n",
    "    car_from_voxel = np.linalg.inv(create_transformation_matrix_to_voxel_space(bev_shape, voxel_size, (0, 0, z_offset)))\n",
    "\n",
    "\n",
    "    global_from_voxel = np.dot(global_from_car, car_from_voxel)\n",
    "    sample_boxes = transform_points(sample_boxes, global_from_voxel)\n",
    "\n",
    "    # We don't know at where the boxes are in the scene on the z-axis (up-down), let's assume all of them are at\n",
    "    # the same height as the ego vehicle.\n",
    "    sample_boxes[2,:] = ego_pose[\"translation\"][2]\n",
    "\n",
    "\n",
    "    # (3, N*4) -> (N, 4, 3)\n",
    "    sample_boxes = sample_boxes.transpose(1,0).reshape(-1, 4, 3)\n",
    "\n",
    "#     box_height = 1.75\n",
    "    box_height = np.array([class_heights[cls] for cls in sample_detection_class])\n",
    "\n",
    "    # Note: Each of these boxes describes the ground corners of a 3D box.\n",
    "    # To get the center of the box in 3D, we'll have to add half the height to it.\n",
    "    sample_boxes_centers = sample_boxes.mean(axis=1)\n",
    "    sample_boxes_centers[:,2] += box_height/2\n",
    "\n",
    "    # Width and height is arbitrary - we don't know what way the vehicles are pointing from our prediction segmentation\n",
    "    # It doesn't matter for evaluation, so no need to worry about that here.\n",
    "    # Note: We scaled our targets to be 0.8 the actual size, we need to adjust for that\n",
    "    sample_lengths = np.linalg.norm(sample_boxes[:,0,:] - sample_boxes[:,1,:], axis=1) * 1/box_scale\n",
    "    sample_widths = np.linalg.norm(sample_boxes[:,1,:] - sample_boxes[:,2,:], axis=1) * 1/box_scale\n",
    "    \n",
    "    sample_boxes_dimensions = np.zeros_like(sample_boxes_centers) \n",
    "    sample_boxes_dimensions[:,0] = sample_widths\n",
    "    sample_boxes_dimensions[:,1] = sample_lengths\n",
    "    sample_boxes_dimensions[:,2] = box_height\n",
    "    \n",
    "    temp = []\n",
    "    for i in range(len(sample_boxes)):\n",
    "        translation = sample_boxes_centers[i]\n",
    "        size = sample_boxes_dimensions[i]\n",
    "        class_name = sample_detection_class[i]\n",
    "        ego_distance = float(np.linalg.norm(ego_translation - translation))\n",
    "    \n",
    "        \n",
    "        # Determine the rotation of the box\n",
    "        v = (sample_boxes[i,0] - sample_boxes[i,1])\n",
    "        v /= np.linalg.norm(v)\n",
    "        r = R.from_dcm([\n",
    "            [v[0], -v[1], 0],\n",
    "            [v[1],  v[0], 0],\n",
    "            [   0,     0, 1],\n",
    "        ])\n",
    "        quat = r.as_quat()\n",
    "        # XYZW -> WXYZ order of elements\n",
    "        quat = quat[[3,0,1,2]]\n",
    "        \n",
    "        detection_score = float(sample_detection_scores[i])\n",
    "\n",
    "        \n",
    "        box3d = Box(\n",
    "            token=sample_token,\n",
    "            center=list(translation),\n",
    "            size=list(size),\n",
    "            orientation=Quaternion(quat),\n",
    "            name=class_name,\n",
    "            score=detection_score\n",
    "        )\n",
    "        \n",
    "        temp.append(box3d)\n",
    "        box3d = Box3D(\n",
    "            sample_token=sample_token,\n",
    "            translation=list(translation),\n",
    "            size=list(size),\n",
    "            rotation=list(quat),\n",
    "            name=class_name,\n",
    "            score=detection_score\n",
    "        )\n",
    "        pred_box3ds.append(box3d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7841e913945b4d57969733392cf19021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=622784), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sub = {}\n",
    "for i in tqdm_notebook(range(len(pred_box3ds))):\n",
    "    yaw = 2*np.arccos(pred_box3ds[i].rotation[0]);\n",
    "    pred =  str(pred_box3ds[i].score/255) + ' ' + str(pred_box3ds[i].center_x)  + ' '  + \\\n",
    "    str(pred_box3ds[i].center_y) + ' '  + str(pred_box3ds[i].center_z) + ' '  + \\\n",
    "    str(pred_box3ds[i].width) + ' ' \\\n",
    "    + str(pred_box3ds[i].length) + ' '  + str(pred_box3ds[i].height) + ' ' + str(yaw) + ' ' \\\n",
    "    + str(pred_box3ds[i].name) + ' ' \n",
    "        \n",
    "    if pred_box3ds[i].sample_token in sub.keys():     \n",
    "        sub[pred_box3ds[i].sample_token] += pred\n",
    "    else:\n",
    "        sub[pred_box3ds[i].sample_token] = pred        \n",
    "    \n",
    "sample_sub = pd.read_csv('/home/ys3152/sample_submission.csv')\n",
    "for token in set(sample_sub.Id.values).difference(sub.keys()):\n",
    "    sub[token] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57b51d24fad1c39065f80c9f769b3ff8d29e17e0cf92f5...</td>\n",
       "      <td>1.0 1747.5730403343164 1274.852777166136 -19.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6d2329e7ccd8ac6469f544145bc971b7ebd69ce44a8db7...</td>\n",
       "      <td>0.9803921568627451 1772.7332509939483 1239.912...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14ca96a81a2c499c150515544b9c40e92c2feba04b63f3...</td>\n",
       "      <td>0.9764705882352941 1775.0584904502116 1238.478...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3c3ed046f64803d5a78282ffedcc6ad04825af17bb33d2...</td>\n",
       "      <td>0.9450980392156862 1777.569088712561 1236.9266...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7a932d3e5d2f58b103b0cc71f24866e0674476e66fcba2...</td>\n",
       "      <td>1.0 1779.8982972888243 1235.4793793216784 -19....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Id  \\\n",
       "0  57b51d24fad1c39065f80c9f769b3ff8d29e17e0cf92f5...   \n",
       "1  6d2329e7ccd8ac6469f544145bc971b7ebd69ce44a8db7...   \n",
       "2  14ca96a81a2c499c150515544b9c40e92c2feba04b63f3...   \n",
       "3  3c3ed046f64803d5a78282ffedcc6ad04825af17bb33d2...   \n",
       "4  7a932d3e5d2f58b103b0cc71f24866e0674476e66fcba2...   \n",
       "\n",
       "                                    PredictionString  \n",
       "0  1.0 1747.5730403343164 1274.852777166136 -19.3...  \n",
       "1  0.9803921568627451 1772.7332509939483 1239.912...  \n",
       "2  0.9764705882352941 1775.0584904502116 1238.478...  \n",
       "3  0.9450980392156862 1777.569088712561 1236.9266...  \n",
       "4  1.0 1779.8982972888243 1235.4793793216784 -19....  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.DataFrame(list(sub.items()))\n",
    "sub.columns = sample_sub.columns\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27463</th>\n",
       "      <td>e7a1900fa84d0c252c4c53ff76b481a6dd32961383e2ea...</td>\n",
       "      <td>1.0 2435.5545878742214 863.8822947404476 -18.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27464</th>\n",
       "      <td>671c813f53d14358bd20d648eb5113facbdeff5d098e70...</td>\n",
       "      <td>1.0 2435.134046289266 864.1103792325146 -18.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27465</th>\n",
       "      <td>147c6cb7fad6de0bb6a565247233ceab160ea212efdd72...</td>\n",
       "      <td>0.9921568627450981 2436.3507264834407 863.1893...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27466</th>\n",
       "      <td>2f00624fd3b1ffcd04976f73a413f65b48a993a5514f1a...</td>\n",
       "      <td>0.9176470588235294 2463.8930633370383 846.7942...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27467</th>\n",
       "      <td>db7eee8f5a0fd8aec6dc62530c3be5b5920cd7fa01c840...</td>\n",
       "      <td>1.0 2476.3948804274714 838.6654588885517 -18.8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Id  \\\n",
       "27463  e7a1900fa84d0c252c4c53ff76b481a6dd32961383e2ea...   \n",
       "27464  671c813f53d14358bd20d648eb5113facbdeff5d098e70...   \n",
       "27465  147c6cb7fad6de0bb6a565247233ceab160ea212efdd72...   \n",
       "27466  2f00624fd3b1ffcd04976f73a413f65b48a993a5514f1a...   \n",
       "27467  db7eee8f5a0fd8aec6dc62530c3be5b5920cd7fa01c840...   \n",
       "\n",
       "                                        PredictionString  \n",
       "27463  1.0 2435.5545878742214 863.8822947404476 -18.0...  \n",
       "27464  1.0 2435.134046289266 864.1103792325146 -18.05...  \n",
       "27465  0.9921568627450981 2436.3507264834407 863.1893...  \n",
       "27466  0.9176470588235294 2463.8930633370383 846.7942...  \n",
       "27467  1.0 2476.3948804274714 838.6654588885517 -18.8...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('/home/hsb2140/lyft3d_pred_pspnet_unet_mean.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
