{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from util import box3d, evatwobox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set the path to groud_truth_file and prediction_file\n",
    "groud_truth_file = '/home/ys3152/val_gt.csv'\n",
    "prediction_file = '/home/ys3152/unet_ensemble_180_val_pred.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id', 'PredictionString']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### read these two csv file row by row and skip the header\n",
    "trainreader = csv.reader(open(groud_truth_file, newline=''))\n",
    "next(trainreader)\n",
    "testreader = csv.reader(open(prediction_file, newline=''))\n",
    "next(testreader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ap(recalls, precisions):\n",
    "    \"\"\"Calculate average precision.\n",
    "    Args:\n",
    "      recalls:\n",
    "      precisions: Returns (float): average precision.\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    # correct AP calculation\n",
    "    # first append sentinel values at the end\n",
    "    recalls = np.concatenate(([0.0], recalls, [1.0]))\n",
    "    precisions = np.concatenate(([0.0], precisions, [0.0]))\n",
    "\n",
    "    precisions = get_envelope(precisions)\n",
    "\n",
    "    # to calculate area under PR curve, look for points where X axis (recall) changes value\n",
    "    i = np.where(recalls[1:] != recalls[:-1])[0]\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    ap = np.sum((recalls[i + 1] - recalls[i]) * precisions[i + 1])\n",
    "    return ap\n",
    "\n",
    "\n",
    "\n",
    "def get_envelope(precisions):\n",
    "    \"\"\"Compute the precision envelope.\n",
    "    Args:\n",
    "      precisions:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    for i in range(precisions.size - 1, 0, -1):\n",
    "        precisions[i - 1] = np.maximum(precisions[i - 1], precisions[i])\n",
    "    return precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b891f1f7c01b4b51bb08385b9f789cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### nrows is used as early stop, for the use of debugging\n",
    "nrows = -1\n",
    "\n",
    "\n",
    "progress_bar = tqdm_notebook(zip(trainreader, testreader))\n",
    "image_score = []\n",
    "ap = []\n",
    "for idx, (row1, row2) in enumerate(progress_bar):\n",
    "    thd_score = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for thd in np.arange(0.5, 1.0, 0.05):\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        \n",
    "        ## convert the raw string data into list of predictions\n",
    "        listpred = row2[1].split(\" \")\n",
    "        predicts = [listpred[int(i)*9: int(i)*9+9] for i in range(int(len(listpred) /9) )]\n",
    "        listgt = row1[1].split(\" \")\n",
    "        gt = [listgt[int(i)*8: int(i)*8+8] for i in range(int(len(listgt) /8) )]\n",
    "        \n",
    "        ## scan of prediction to see if there is a box hit\n",
    "        for i in range(len(predicts)):\n",
    "            for j in range(len(gt)):\n",
    "                if evatwobox(predicts[i], gt[j], thd):\n",
    "                    if predicts[i][8] == gt[j][7]:\n",
    "                        tp+=1\n",
    "                        gt[j][7]=\"used\"\n",
    "                        ## note that a single ground truth box cannot be used by multiple predictions\n",
    "                    else:\n",
    "                        fp+=1\n",
    "        fn = int(len(gt)) - tp\n",
    "        score = tp / (tp + fp + fn + 0.001)\n",
    "        precision.append(tp / (tp + fp + 0.001))\n",
    "        recall.append(tp / (tp + fn + 0.001))\n",
    "        ### score is true positive / (true positive + false positive + false negative)\n",
    "        ### the 0.001 is used to prevent dividing by 0 from happening\n",
    "        thd_score.append(score)\n",
    "        \n",
    "    ap.append(get_ap(recall, precision))\n",
    "    image_score.append(np.mean(thd_score))\n",
    "    if nrows != -1 and idx == nrows:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The score is : {}\".format(np.mean(image_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mAP is : {}\".format(np.mean(ap)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
